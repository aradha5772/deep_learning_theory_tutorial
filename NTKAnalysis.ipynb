{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5992bf0f",
   "metadata": {},
   "source": [
    "# NTK Derivation and Analysis\n",
    "\n",
    "In this Notebook, we will derive a closed form for the NTK for 1 hidden layer ReLU networks.  We will then present experiments to show that the NTK can be used to describe the behavior of large width neural networks.  \n",
    "\n",
    "We begin with a derivation of the NTK below (this is basically the solution to Section 2 Problem 2 of the worksheet shared in the github).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbae44",
   "metadata": {},
   "source": [
    "## Derivation of the NTK \n",
    "\n",
    "Suppose we are given a dataset $\\{(x^{(i)}, y^{(i)}\\}_{i=1}^{n} \\subset \\mathbb{R}^{d} \\times \\mathbb{R}$ (also written as $X \\in \\mathbb{R}^{d \\times n}, y \\in \\mathbb{R}^{1 \\times n}$).  Let $f$ denote a 1 hidden layer neural network with parameters $\\mathbf{W}$.  To train the neural network $f$ to fit the data $(X, y)$, we typically use gradient descent to minimize the following loss: \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\mathbf{W}) = \\sum_{i=1}^{n} (y^{(i)} - f(\\mathbf{W} ; x^{(i)}))^2 \n",
    "\\end{align*}\n",
    "\n",
    "**Important:** Note that the network $f$ is written as a function of parameters $\\mathbf{W}$ and data $x^{(i)}$, as opposed to just data.  For the neural tangent kernel derivation, we consider the cross section of $f$ given by fixing the data component and writing the neural network as a function of parameters, i.e. consider $f_x(\\mathbf{W}): \\mathbb{R}^{dk + k} \\to \\mathbb{R}$.  \n",
    "\n",
    "### Linearization around Initialization\n",
    "Before training the network as usual, let us consider the following alternative.  Viewing the neural network as only a function of parameters, we train the linear approximation for $f_x(\\mathbf{W})$, which is given as follows: \n",
    "\n",
    "\\begin{align*}\n",
    "\\tilde{f_x}(\\mathbf{W}) = f_x(\\mathbf{W}^{(0)}) + \\nabla f_x(\\mathbf{W}^{(0)})^T (\\mathbf{W} - \\mathbf{W}^{(0)}) ~;\n",
    "\\end{align*}\n",
    "where $\\mathbf{W}^{(0)} \\in \\mathbb{R}^{dk + k}$ denotes the parameters at initialization and $\\nabla f_x(\\mathbf{W}^{(0)})^T \\in \\mathbb{R}^{1 \\times (dk + k)}$ denotes the gradient of $f_x(\\mathbf{W})$.  Instead of minimizing the loss for the model $f(\\mathbf{W} ; x^{(i)})$ given above, we instead minimize the following loss: \n",
    "\n",
    "\\begin{align*}\n",
    "    \\tilde{\\mathcal{L}}(\\mathbf{W}) = \\sum_{i=1}^{n} (y^{(i)} - \\tilde{f}_{x^{(i)}}(\\mathbf{W}))^2 = \\sum_{i=1}^{n} (y^{(i)} - f_{x^{(i)}}(\\mathbf{W}^{(0)}) - \\nabla f_{x^{(i)}}(\\mathbf{W}^{(0)})^T (\\mathbf{W} - \\mathbf{W}^{(0)}))^2\n",
    "\\end{align*}\n",
    "\n",
    "Minimizing this loss naively can be computationally expensive since the vector $\\mathbf{W} \\in \\mathbb{R}^{kd + k}$  depends on $k$, which can be arbitrarily large.  To remedy this, we let $\\mathbf{W} = \\mathbf{W}^{(0)} + \\sum_{i=1}^{n} \\nabla f_{x^{(i)}}(\\mathbf{W}^{(0)})\\alpha_i$. \n",
    "\n",
    "\n",
    "**Remark:** At this point, you should be asking why this is a reasonable step to take.  The rationale for this step is that we can use this to find the minimum norm minizimer, which lies in the span of the training data.  If you haven't seen this trick before, I encourage you to review the Representer theorem.  \n",
    "\n",
    "Using the new form for $\\mathbf{W}$, we can simplify our loss $\\tilde{\\mathcal{L}}(\\mathbf{W})$ as follows: \n",
    "\\begin{align*}\n",
    "\\tilde{\\mathcal{L}}(\\mathbf{W}) = \\sum_{i=1}^{n} (y^{(i)} - f_{x^{(i)}}(\\mathbf{W}^{(0)}) - \\alpha k(x^{(i)}) )^2 ~;\n",
    "\\end{align*}\n",
    "where $\\alpha \\in \\mathbb{R}^{1 \\times n}$ and $$k(x) = \\begin{bmatrix} \\langle \\nabla f_{x}(\\mathbf{W}^{(0)}),  \\nabla f_{x^{(1)}}(\\mathbf{W}^{(0)}) \\rangle \\\\ \\langle \\nabla f_{x}(\\mathbf{W}^{(0)}),  \\nabla f_{x^{(2)}}(\\mathbf{W}^{(0)}) \\rangle \\\\ \\vdots \\\\ \\langle \\nabla f_{x}(\\mathbf{W}^{(0)}),  \\nabla f_{x^{(n)}}(\\mathbf{W}^{(0)}) \\rangle  \\end{bmatrix} \\in \\mathbb{R}^{n}$$\n",
    "\n",
    "We can now recognize minimizing the loss $\\tilde{\\mathcal{L}}(\\mathbf{W})$ as solving the following system of equations: \n",
    "\\begin{align*}\n",
    " \\alpha K = y - f_X(\\mathbf{W}^{(0)}) ~;\n",
    "\\end{align*}\n",
    "where $K \\in \\mathbb{R}^{n \\times n}$ with $K_{i,j} = \\langle \\nabla f_{x^{(i)}}(\\mathbf{W}^{(0)}),  \\nabla f_{x^{(j)}}(\\mathbf{W}^{(0)}) \\rangle$ and $f_X(\\mathbf{W}^{(0)}) \\in \\mathbb{R}^{1 \\times n}$ with $f_X(\\mathbf{W}^{(0)})_i = f_{x^{(i)}}(\\mathbf{W}^{(0)})$.  \n",
    "\n",
    "**Definition [NTK]:** The function $K_{i,j}$ above is written generally as the following Neural Tangent Kernel:\n",
    "$$ K(x, x') = \\langle \\nabla f_{x}(\\mathbf{W}^{(0)}), \\nabla f_{x'}(\\mathbf{W}^{(0)}) \\rangle $$.  \n",
    "\n",
    "**Remarks:** This kernel can of course be evaluated using any auto-differentition software (e.g. PyTorch, Tensorflow, Jax, etc.).  This is generally memory (and runtime) expensive since neural networks can have millions or billions of parameters.  On the other hand, we can actually analytically compute the kernel $K$ when the width of neural networks approaches infinity.  We do this below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f964c28",
   "metadata": {},
   "source": [
    "### Analytical Evaluation of the NTK (1 Hidden Layer,)\n",
    "Thus far, we have defined the NTK without explicitly computing it for a given architecture.  We now write a closed form for the NTK given a specific archticture.  In particular, let $f$ denote a 1 hidden layer network defined as follows: \n",
    "\\begin{align*}\n",
    "    f(\\mathbf{W} ; x) = a \\frac{\\sqrt{c}}{\\sqrt{k}} \\phi(Bx) ~;\n",
    "\\end{align*}\n",
    "where $a \\in \\mathbb{R}^{1 \\times k}, B \\in \\mathbb{R}^{k \\times d}$ are the trainable parameters ($\\mathbf{W} = [a_1, a_2, \\ldots a_k, B_{1,1}, B_{1,2}, \\ldots B_{k, d}]^T \\in \\mathbb{R}^{k + dk}$ denotes the vector containing all trainable parameters), $c \\in \\mathbb{R}$ is an absolute constant, and $\\phi: \\mathbb{R} \\to \\mathbb{R}$ is an elementwise nonlinearity.  \n",
    "\n",
    "Let us now compute the NTK $K(x, x') = \\langle \\nabla f_{x}(\\mathbf{W}^{(0)}), \\nabla f_{x'}(\\mathbf{W}^{(0)}) \\rangle$ as $k \\to \\infty$ assuming that $\\mathbf{W}_j^{(0)} \\overset{i.i.d.}{\\sim} \\mathcal{N}(0, 1)$.  Letting $\\mathbf{W} = [a_1, a_2, \\ldots a_k, B_{1,1}, B_{1,2}, \\ldots B_{k, d}] $, we compute $\\nabla f_{x}(\\mathbf{W}^{(0)})$ as follows: \n",
    "\n",
    "\\begin{align*}\n",
    "    \\nabla f_{x}(\\mathbf{W}) = \\begin{bmatrix}\\frac{\\partial f_{x}}{\\partial a_1}  \\\\ \\frac{\\partial f_{x}}{\\partial a_2} \\\\ \\vdots \\\\ \\frac{\\partial f_{x}}{\\partial a_k} \\\\ \\frac{\\partial f_{x}}{\\partial B_{1,1}} \\\\ \\vdots \\\\ \\frac{\\partial f_{x}}{\\partial B_{k, d}}\n",
    "   \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "We thus first calculate $\\frac{\\partial f_{x}}{\\partial a_j}$ and $\\frac{\\partial f_{x}}{\\partial B_{j, \\ell}}$: \n",
    "\\begin{align*}\n",
    "    \\frac{\\partial f_{x}}{\\partial a_j} = \\frac{\\sqrt{c}}{\\sqrt{k}} \\phi(B_{j, :}  x) \\\\\n",
    "    \\frac{\\partial f_{x}}{\\partial B_{j, \\ell}} = a_j \\frac{\\sqrt{c}}{\\sqrt{k}} \\phi'(B_{j,:}x) x_{\\ell}    \n",
    "\\end{align*}\n",
    "\n",
    "Now that we have all the relevant terms to compute $\\nabla f_x(\\mathbf{W}^{(0)})$, we can compute $K(x, x')$ as follows: \n",
    "\\begin{align*}\n",
    "    K(x, x') &= \\langle \\nabla f_{x}(\\mathbf{W}^{(0)}), \\nabla f_{x'}(\\mathbf{W}^{(0)}) \\rangle \\\\\n",
    "    &= \\sum_{j=1}^{k} \\frac{\\partial f_x(\\mathbf{W}^{(0)})}{\\partial a_j} \\frac{\\partial f_{x'}(\\mathbf{W}^{(0)})}{\\partial a_j} + \\sum_{j=1}^{k} \\sum_{\\ell = 1}^{d} \\frac{\\partial f_x(\\mathbf{W}^{(0)})}{\\partial B_{j, \\ell}} \\frac{\\partial f_{x'}(\\mathbf{W}^{(0)})}{\\partial B_{j, \\ell}} \\\\\n",
    "    &= \\color{red}{\\text{$\\frac{c}{k} \\sum_{j=1}^{k}  \\phi(B_{j, :}  x) \\phi(B_{j, :}  x')$}} ~ + ~ \\color{blue}{\\text{$\\frac{c}{k} \\sum_{j=1}^{k} \\sum_{\\ell=1}^{d} a_j^2  \\phi'(B_{j, :}  x) \\phi'(B_{j, :}  x') x_{\\ell} x'_{\\ell}$}}  \\\\\n",
    "    &= \\color{red}{\\text{$\\frac{c}{k} \\sum_{j=1}^{k}  \\phi(B_{j, :}  x) \\phi(B_{j, :}  x')$}} ~ + ~ \\color{blue}{\\text{$\\frac{c}{k} \\sum_{j=1}^{k}  a_j^2 \\phi'(B_{j, :}  x) \\phi'(B_{j, :}  x')  \\sum_{\\ell=1}^{d} x_{\\ell} x'_{\\ell}$}}  \\\\\n",
    "    &= \\color{red}{\\text{$\\frac{c}{k} \\sum_{j=1}^{k}  \\phi(B_{j, :}  x) \\phi(B_{j, :}  x')$}} ~ + ~ \\langle x, x' \\rangle \\color{blue}{\\text{$\\frac{c}{k} \\sum_{j=1}^{k}  a_j^2 \\phi'(B_{j, :}  x) \\phi'(B_{j, :}  x') $}} \n",
    "\\end{align*}\n",
    "\n",
    "**Remark:** Do the red and blue terms look familiar? If you worked through the notebook *DoubleDescentTutorial*, they should.  Indeed, as $k \\to \\infty$, the terms in the red and blue correspond to the NNGP kernel for a network with activation $\\phi$ and $\\phi'$ respectively.  We know how to evaluate these using dual activations.  Namely, we have: \n",
    "\\begin{align*}\n",
    "    \\color{red}{\\text{$\\frac{c}{k} \\sum_{j=1}^{k}  \\phi(B_{j, :}  x) \\phi(B_{j, :}  x')$}} &\\to c \\mathbb{E}_{(u, v) \\sim \\mathcal{N}(\\mathbf{0}, \\Lambda)} [\\phi(u) \\phi(v) ] \\\\\n",
    "    \\color{blue}{\\text{$\\frac{c}{k} \\sum_{j=1}^{k}  a_j^2 \\phi'(B_{j, :}  x) \\phi'(B_{j, :}  x') $}} &\\to c \\mathbb{E}_{(u, v) \\sim \\mathcal{N}(\\mathbf{0}, \\Lambda)} [\\phi'(u) \\phi'(v)] \\\\\n",
    "    \\Lambda &= \\begin{bmatrix} \\|x\\|_2^2  & x^T x' \\\\ x^T x' & \\|x'\\|_2^2 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Let $\\xi = x^T x'$ and $\\check{\\phi}$ denote the dual of $\\phi$.  Assuming $\\phi$ is homogeneous of degree 1 and that $\\|x\\|_2 = \\|x'\\|_2 = 1$ we conclude: \n",
    "\\begin{align*}\n",
    "    K(x, x') = \\check{\\phi}(\\xi) + \\xi \\check{\\phi'}(\\xi)\n",
    "\\end{align*}\n",
    "\n",
    "Recalling that the dual activation is computed in closed form for a number of nonlinearities including ReLU, we now have a closed form for the NTK.  Next, let's try training some simple neural networks to verify that the NTK does describe the training dynamics of large neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30982ecb",
   "metadata": {},
   "source": [
    "## Training Neural Nets vs. Using the NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fb4a3f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100) (32, 1) (100, 100) (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# Loading high dimensional linear data\n",
    "import dataloader as dl\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 2134\n",
    "\n",
    "np.random.seed(SEED)\n",
    "d = 100\n",
    "n = 32\n",
    "n_test = 100\n",
    "\n",
    "X = np.random.randn(n, d)\n",
    "X = X / norm(X, axis=-1).reshape(-1, 1)\n",
    "X_test = np.random.randn(n_test, d)\n",
    "X_test = X_test / norm(X_test, axis=-1).reshape(-1, 1)\n",
    "w = np.random.randn(1, d)\n",
    "y = (w @ X.T).T\n",
    "y_test = (w @ X_test.T).T\n",
    "print(X.shape, y.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c1ae559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We now need to define and train a neural network to map x^{(i)} to y^{(i)}\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Abstraction for nonlinearity \n",
    "class Nonlinearity(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Nonlinearity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return F.leaky_relu(x)\n",
    "        return F.relu(x)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, width, f_in):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.k = width\n",
    "        self.first = nn.Sequential(nn.Linear(f_in, self.k, bias=True), \n",
    "                                   Nonlinearity())\n",
    "        self.sec = nn.Linear(self.k, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #C = np.sqrt(2/(.01**2 + 1)) * 1/np.sqrt(self.k)\n",
    "        C = np.sqrt(2/self.k)\n",
    "        o = self.first(x) * C\n",
    "        return self.sec(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3e5cc8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16000, 100])\n",
      "torch.Size([16000])\n",
      "torch.Size([1, 16000])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc04b360b5454c97ab466ddb2d35d864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Loss:  2.73050878549734\n",
      "Epoch  1000  Loss:  0.09766805617471178\n",
      "Epoch  2000  Loss:  0.012308375464602797\n",
      "Epoch  3000  Loss:  0.0021917035476749203\n",
      "Epoch  4000  Loss:  0.0004582773470279824\n",
      "Epoch  5000  Loss:  0.00010388931426187457\n",
      "Epoch  6000  Loss:  2.4736101571105667e-05\n",
      "Epoch  7000  Loss:  6.091727017858584e-06\n",
      "Epoch  8000  Loss:  1.5372611678632235e-06\n",
      "Epoch  9000  Loss:  3.950678705827788e-07\n",
      "Epoch  10000  Loss:  1.0296731246989766e-07\n",
      "Epoch  11000  Loss:  2.7142207903909124e-08\n",
      "Epoch  12000  Loss:  7.2215891427776104e-09\n",
      "Epoch  13000  Loss:  1.9368270254195775e-09\n",
      "Epoch  14000  Loss:  5.231585017011714e-10\n",
      "Epoch  15000  Loss:  1.422313385276015e-10\n",
      "Finished Width:  16000 End Train Error:  9.990888168618902e-11 End Test Error:  1.0644498390149257 Best Test Error:  1.0644498390149257\n",
      "[array(1.06444984)]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from copy import deepcopy\n",
    "from auto_tqdm import tqdm\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "eps = 1e-10  # Threshold for stopping training\n",
    "\n",
    "# Moving the data to GPU\n",
    "X_t = torch.from_numpy(X).cuda()\n",
    "y_t = torch.from_numpy(y).cuda()\n",
    "\n",
    "X_test_t = torch.from_numpy(X_test).cuda()\n",
    "y_test_t = torch.from_numpy(y_test).cuda()\n",
    "\n",
    "n, d = X.shape\n",
    "\n",
    "widths = [16000]\n",
    "test_errors = []\n",
    "best_test_errors = []\n",
    "networks = []\n",
    "\n",
    "for width in widths:\n",
    "\n",
    "    # Create our network\n",
    "    net = Net(width, d)\n",
    "    \n",
    "    # Initialize the parameters i.i.d. from a standard normal \n",
    "    for idx, param in enumerate(net.parameters()):\n",
    "        print(param.size())\n",
    "        init = torch.Tensor(param.size()).normal_()\n",
    "        param.data = init\n",
    "    net.double()\n",
    "    net_0 = deepcopy(net)\n",
    "    net.cuda()\n",
    "\n",
    "    # Training neural network with GD\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=1e-2)\n",
    "\n",
    "    epochs = 100000\n",
    "\n",
    "    best_test_error = float(\"inf\")\n",
    "    for i in tqdm(range(epochs), total=epochs):\n",
    "        net.zero_grad()\n",
    "        preds = net(X_t)\n",
    "        loss = torch.mean(torch.pow(preds - y_t, 2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.cpu().data.numpy()\n",
    "        if loss < 1e-10: \n",
    "            break \n",
    "        if i % 1000 == 0:\n",
    "            print(\"Epoch \", i, \" Loss: \", loss)\n",
    "        test_error = torch.mean(torch.pow(net(X_test_t) - y_test_t, 2)).cpu().data.numpy()\n",
    "        best_test_error = min(best_test_error, test_error)\n",
    "\n",
    "    print(\"Finished Width: \", width, \"End Train Error: \", loss, \"End Test Error: \", test_error, \n",
    "          \"Best Test Error: \", best_test_error)\n",
    "    best_test_errors.append(best_test_error)\n",
    "    test_errors.append(test_error)\n",
    "    networks.append(deepcopy(net.cpu()))\n",
    "print(test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2ead63a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Compute Empirical NTK\n",
    "from auto_tqdm import tqdm\n",
    "\n",
    "preds = net_0(X_t.cpu())\n",
    "n, d = X.shape\n",
    "\n",
    "l = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "K = np.zeros((n, n))\n",
    "all_grads = np.zeros((n, l))\n",
    "\n",
    "preds = net_0(X_t.cpu())    \n",
    "\n",
    "for idx in tqdm(range(n), total=n):\n",
    "    net_0.zero_grad()\n",
    "    p = net_0(X_t.cpu())[idx]\n",
    "    p.backward()\n",
    "    grads = [q.grad.flatten() for q in net_0.parameters()]\n",
    "    grads = torch.cat(grads)\n",
    "    all_grads[idx,:] = deepcopy(grads.numpy())\n",
    "\n",
    "\n",
    "for i in tqdm(range(n), total=n):\n",
    "    for j in range(n):\n",
    "        K[i, j] = np.sum(all_grads[i, :] * all_grads[j, :])\n",
    "K_empirical = K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3d4b95f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.linalg import pinv\n",
    "\n",
    "K_inv = pinv(K)\n",
    "sol = y.T @ K_inv\n",
    "\n",
    "n_test, d = X_test.shape\n",
    "K_test = np.zeros((n, n_test))\n",
    "for j in tqdm(range(n_test), total=n_test):\n",
    "    net_0.zero_grad()\n",
    "    p = net_0(X_test_t.cpu())[j]\n",
    "    p.backward()\n",
    "    grads = [q.grad.flatten() for q in net_0.parameters()]\n",
    "    grads = torch.cat(grads)\n",
    "    for i, g in enumerate(all_grads):\n",
    "        K_test[i, j] = np.sum(g * grads.numpy())\n",
    "\n",
    "f_X = net_0(X_t.cpu()).data.numpy()\n",
    "f_x = net_0(X_test_t.cpu()).data.numpy()\n",
    "\n",
    "empirical_pred_test_corrected = (y.T - f_X.T) @ (K_inv @ K_test) + f_x.T\n",
    "empirical_pred_test = y.T @ K_inv @ K_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "36fe4062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical NTK: \n",
      " [[4.         1.79555843 1.64980051 ... 1.94070056 1.84797068 2.06689513]\n",
      " [1.79555843 4.         1.81279459 ... 1.90982142 2.0879102  1.97895885]\n",
      " [1.64980051 1.81279459 4.         ... 2.05568052 1.9516431  2.14718829]\n",
      " ...\n",
      " [1.94070056 1.90982142 2.05568052 ... 4.         1.87065453 2.24250602]\n",
      " [1.84797068 2.0879102  1.9516431  ... 1.87065453 4.         1.65249391]\n",
      " [2.06689513 1.97895885 2.14718829 ... 2.24250602 1.65249391 4.        ]]\n",
      "Empirical NTK: \n",
      " [[4.0561158  1.84060637 1.68579264 ... 1.98192256 1.8430595  2.13493269]\n",
      " [1.84060637 4.06273434 1.86152534 ... 1.94122551 2.10153732 2.05116363]\n",
      " [1.68579264 1.86152534 4.03538226 ... 2.10823912 1.9714356  2.21346735]\n",
      " ...\n",
      " [1.98192256 1.94122551 2.10823912 ... 4.07341524 1.86327991 2.31460805]\n",
      " [1.8430595  2.10153732 1.9714356  ... 1.86327991 3.97119779 1.67430362]\n",
      " [2.13493269 2.05116363 2.21346735 ... 2.31460805 1.67430362 4.12117518]]\n",
      "1.2598217952903903e-29 0.48437464815921794\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import solve\n",
    "\n",
    "def mse(preds, labels): \n",
    "    return np.mean(np.abs(np.power(preds - labels, 2)))\n",
    "\n",
    "# Infinite Width Random ReLU Feature Regression\n",
    "def ntk(pair1, pair2):\n",
    "\n",
    "    out = pair1 @ pair2.transpose(1, 0) + 1\n",
    "    N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "    N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "    XX = np.sqrt(N1 @ N2.transpose(1, 0))\n",
    "    out = out / XX\n",
    "\n",
    "    out = np.clip(out, a_min=-1, a_max=1)\n",
    "\n",
    "    first = 1/np.pi * (out * (np.pi - np.arccos(out)) \\\n",
    "                           + np.sqrt(1. - np.power(out, 2))) * XX\n",
    "    sec = 1/np.pi * out * (np.pi - np.arccos(out)) * XX\n",
    "    out = first + sec\n",
    "    return out \n",
    "\n",
    "# Build kernel matrix for train & test data\n",
    "K_train = ntk(X, X) #+ np.eye(X.shape[0])*.0001\n",
    "K_test = ntk(X, X_test)\n",
    "\n",
    "# Solve kernel regression\n",
    "K_inv = pinv(K_train) \n",
    "a_hat = y.T @ K_inv \n",
    "\n",
    "print(\"Theoretical NTK: \\n\", K_train)\n",
    "\n",
    "print(\"Empirical NTK: \\n\", K_empirical)\n",
    "\n",
    "theoretical_pred_test = a_hat @ K_test\n",
    "\n",
    "# Get error on train & test data\n",
    "train_error = mse(a_hat @ ntk(X,X), y.T)\n",
    "test_error =mse(a_hat @ K_test, y_test.T)\n",
    "print(train_error, test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6e79c084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1682423  -0.05881645  0.02397388  0.1342176  -0.34087719]\n",
      "[-0.1874467  -0.06801235 -0.01552141  0.18810081 -0.3450094 ]\n",
      "0.0005990180311231504\n"
     ]
    }
   ],
   "source": [
    "num = 5\n",
    "print(theoretical_pred_test[0,:num])\n",
    "print(empirical_pred_test[0, :num])\n",
    "print(mse(empirical_pred_test, theoretical_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1e2b386e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infinite width: \n",
      " [-0.81182529  0.79999013  0.11211074 -0.46587294  0.49494158]\n",
      "Empirical NTK Corrected: \n",
      " [-0.80058685  0.79097463  0.03352433 -0.40223674  0.48549821]\n",
      "Ground Truth: \n",
      " [-0.80659439  0.78833474  0.03661104 -0.41004607  0.48631202]\n",
      "Error between trained net and NTK:  0.0011547295683387625\n",
      "Error between trained net and empirical NTK 1.4939568621191908e-05\n"
     ]
    }
   ],
   "source": [
    "gt_preds = net(X_test_t.cpu()).data.numpy().T\n",
    "f_X = net_0(X_t.cpu()).data.numpy()\n",
    "\n",
    "\n",
    "def theoretical_pred(x):\n",
    "    k_x = ntk(X, x)\n",
    "    f_x = net_0(torch.from_numpy(x)).data.numpy().T\n",
    "    return (y.T  - f_X.T) @ (K_inv @ k_x) + f_x\n",
    "\n",
    "theoretical_pred_test_corrected = theoretical_pred(X_test)\n",
    "num = 5\n",
    "\n",
    "print(\"Infinite width: \\n\", theoretical_pred_test_corrected[0, :num])\n",
    "print(\"Empirical NTK Corrected: \\n\", empirical_pred_test_corrected[0, :num])\n",
    "print(\"Ground Truth: \\n\", gt_preds[0, :num])\n",
    "print(\"Error between trained net and NTK: \", \n",
    "      mse(theoretical_pred_test_corrected, gt_preds))\n",
    "print(\"Error between trained net and empirical NTK\", \n",
    "      mse(empirical_pred_test_corrected,gt_preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_tutorial",
   "language": "python",
   "name": "dl_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
