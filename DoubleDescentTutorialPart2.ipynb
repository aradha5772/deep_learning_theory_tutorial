{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Descent with 1 Hidden Layer Neural Networks \n",
    "\n",
    "In this notebook, we identify the double descent phenomenon when training 1 hidden layer neural networks.  Again, given a dataset $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^{n} \\subset \\mathbb{R}^{d} \\times \\mathbb{R}$, we wish to learn a map from $x^{(i)} \\to y^{(i)}$.  To learn such a map, we use the following 1 hidden layer nonlinear network: \n",
    "\\begin{align*}\n",
    "    f(\\mathbf{W} ; x) = a \\frac{\\sqrt{c}}{\\sqrt{k}} \\phi(B x) ~~;\n",
    "\\end{align*}\n",
    "where $a \\in \\mathbb{R}^{1 \\times k}$, $B \\in \\mathbb{R}^{k \\times d}$, $x \\in \\mathbb{R}^{d}$, $c \\in \\mathbb{R}$ is a fixed constant, $\\phi$ is an elementwise nonlinearity, and $\\mathbf{W}$ is a vectorized version of all entries of $a, B$ (e.g. $\\mathbf{W} \\in \\mathbb{R}^{k + dk}$).  We will also assume that $\\phi$ is a real valued function (as is the case in many models in practice).  \n",
    "\n",
    "\n",
    "We will assume that the parameters $\\mathbf{W}_i \\overset{i.i.d}{\\sim} \\mathcal{N}(0, 1)$.  We then use gradient descent to minimize the following loss: \n",
    "\\begin{align}\n",
    "    \\mathcal{L}(w) = \\sum_{i=1}^{n} ( y^{(i)} - f(x^{(i)}))^2  ~~;\n",
    "\\end{align}\n",
    "\n",
    "We will now show that double descent occurs when the number of hidden units $k$ increases.  \n",
    "\n",
    "**Note:** The following code will make use of the GPU (it can still run without the GPU, but will take a bit longer).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aradha/anaconda3/envs/dl_tutorial/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448265233/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:  torch.Size([4000, 784])\n",
      "Train Labels:  torch.Size([4000, 10])\n",
      "Test Set:  torch.Size([10000, 784])\n",
      "Test Labels:  torch.Size([10000, 10])\n"
     ]
    }
   ],
   "source": [
    "# We will use a subset of MNIST for demonstrating double descent \n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "train_set = datasets.MNIST('./data', train=True, download=True)\n",
    "test_set = datasets.MNIST('./data', train=False, download=True)\n",
    "\n",
    "# Loading/Normalizing training & test images\n",
    "train_imgs, train_labels = train_set.data / 256, train_set.targets\n",
    "test_imgs, test_labels = test_set.data / 256, test_set.targets\n",
    "\n",
    "classes = {}\n",
    "max_per_class = 400\n",
    "max_labels = 10\n",
    "\n",
    "for idx, label in enumerate(train_labels): \n",
    "    label = label.data.numpy().item()\n",
    "    if label in classes and len(classes[label]) < max_per_class: \n",
    "        classes[label].append(train_imgs[idx])\n",
    "    elif label not in classes: \n",
    "        classes[label] = [train_imgs[idx]]\n",
    "        \n",
    "    if len(classes) >= max_labels:\n",
    "        early_exit = True\n",
    "        for label in classes: \n",
    "            early_exit &= len(classes[label]) >= max_per_class\n",
    "        if early_exit: \n",
    "            break\n",
    "\n",
    "all_train_examples = []\n",
    "all_train_labels = []\n",
    "for label in classes:\n",
    "    label_vec = torch.zeros(max_labels)\n",
    "    label_vec[label] = 1.\n",
    "    all_train_examples.extend(classes[label])\n",
    "    all_train_labels.extend([label_vec]*len(classes[label]))\n",
    "    \n",
    "all_test_labels = []    \n",
    "for label in test_labels: \n",
    "    label = label.data.numpy().item()\n",
    "    label_vec = torch.zeros(max_labels)\n",
    "    label_vec[label] = 1.\n",
    "    all_test_labels.append(label_vec)\n",
    "    \n",
    "    \n",
    "train_set = torch.stack(all_train_examples, dim=0).view(max_labels * max_per_class, -1)\n",
    "train_set = train_set / torch.norm(train_set, p=2, dim=1).view(-1, 1)\n",
    "train_labels = torch.stack(all_train_labels, dim=0)\n",
    "\n",
    "test_set = test_imgs.view(-1, 28*28)\n",
    "test_set = test_set / torch.norm(test_set, p=2, dim=1).view(-1, 1) \n",
    "test_labels = torch.stack(all_test_labels, dim=0)\n",
    "\n",
    "print(\"Train Set: \", train_set.shape)\n",
    "print(\"Train Labels: \", train_labels.shape)\n",
    "print(\"Test Set: \", test_set.shape)\n",
    "print(\"Test Labels: \", test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network for MNIST Classification\n",
    "\n",
    "Below we provide code for constructing a 1 hidden layer network of width $k$ in PyTorch.  We will consider networks with a bias term in the hidden layer just as in teh previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## We now need to define and train a neural network to map x^{(i)} to y^{(i)}\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Abstraction for nonlinearity \n",
    "class Nonlinearity(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Nonlinearity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        #return F.leaky_relu(x)\n",
    "        return F.relu(x)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, width):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.k = width\n",
    "        self.first = nn.Sequential(nn.Linear(784, self.k, bias=True), \n",
    "                                   Nonlinearity())\n",
    "        self.sec = nn.Linear(self.k, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #C = np.sqrt(2/(.01**2 + 1)) * 1/np.sqrt(self.k)\n",
    "        C = np.sqrt(2/self.k)\n",
    "        o = self.first(x) * C\n",
    "        return self.sec(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a neural network with gradient descent\n",
    "\n",
    "Below, we provide code to train neural networks of varying width to classify $4000$ MNIST digits using gradient descent.  We chose to run gradient descent for $10^5$ epochs to minimize the training loss and accuracy as much as possible.  In practice, we would just early stop the code when the validation accuracy stops improving.  The code below takes too much time to run in the tutorial, but you are encouraged to try it out offline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters:  12720\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.80767\t Train Error: 0.87625\t Test Error: 0.88730 (0 epochs)\t\n",
      "Train MSE: 0.04818\t Train Error: 0.42900\t Test Error: 0.43960 (10000 epochs)\t\n",
      "Train MSE: 0.03938\t Train Error: 0.33850\t Test Error: 0.36380 (20000 epochs)\t\n",
      "Train MSE: 0.03689\t Train Error: 0.32450\t Test Error: 0.35590 (30000 epochs)\t\n",
      "Train MSE: 0.03614\t Train Error: 0.32125\t Test Error: 0.35810 (40000 epochs)\t\n",
      "Train MSE: 0.03561\t Train Error: 0.31975\t Test Error: 0.36130 (50000 epochs)\t\n",
      "Train MSE: 0.03523\t Train Error: 0.31900\t Test Error: 0.36310 (60000 epochs)\t\n",
      "Train MSE: 0.03498\t Train Error: 0.31850\t Test Error: 0.36520 (70000 epochs)\t\n",
      "Train MSE: 0.03469\t Train Error: 0.31800\t Test Error: 0.36750 (80000 epochs)\t\n",
      "Train MSE: 0.03454\t Train Error: 0.31775\t Test Error: 0.37050 (90000 epochs)\t\n",
      "Number of Parameters:  25440\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.65443\t Train Error: 0.89650\t Test Error: 0.90130 (0 epochs)\t\n",
      "Train MSE: 0.03865\t Train Error: 0.33350\t Test Error: 0.34910 (10000 epochs)\t\n",
      "Train MSE: 0.03681\t Train Error: 0.32325\t Test Error: 0.34540 (20000 epochs)\t\n",
      "Train MSE: 0.03586\t Train Error: 0.32000\t Test Error: 0.34680 (30000 epochs)\t\n",
      "Train MSE: 0.03518\t Train Error: 0.31725\t Test Error: 0.34930 (40000 epochs)\t\n",
      "Train MSE: 0.02923\t Train Error: 0.23650\t Test Error: 0.28220 (50000 epochs)\t\n",
      "Train MSE: 0.02648\t Train Error: 0.22600\t Test Error: 0.27850 (60000 epochs)\t\n",
      "Train MSE: 0.02572\t Train Error: 0.22125\t Test Error: 0.28150 (70000 epochs)\t\n",
      "Train MSE: 0.02515\t Train Error: 0.21925\t Test Error: 0.28370 (80000 epochs)\t\n",
      "Train MSE: 0.02474\t Train Error: 0.21750\t Test Error: 0.28800 (90000 epochs)\t\n",
      "Number of Parameters:  31800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.59527\t Train Error: 0.88200\t Test Error: 0.90040 (0 epochs)\t\n",
      "Train MSE: 0.02085\t Train Error: 0.08275\t Test Error: 0.11860 (10000 epochs)\t\n",
      "Train MSE: 0.01559\t Train Error: 0.05550\t Test Error: 0.10420 (20000 epochs)\t\n",
      "Train MSE: 0.01400\t Train Error: 0.04775\t Test Error: 0.10600 (30000 epochs)\t\n",
      "Train MSE: 0.01293\t Train Error: 0.04225\t Test Error: 0.10920 (40000 epochs)\t\n",
      "Train MSE: 0.01208\t Train Error: 0.03725\t Test Error: 0.11360 (50000 epochs)\t\n",
      "Train MSE: 0.01140\t Train Error: 0.03375\t Test Error: 0.11960 (60000 epochs)\t\n",
      "Train MSE: 0.01088\t Train Error: 0.03050\t Test Error: 0.12540 (70000 epochs)\t\n",
      "Train MSE: 0.01047\t Train Error: 0.02800\t Test Error: 0.13020 (80000 epochs)\t\n",
      "Train MSE: 0.00994\t Train Error: 0.02775\t Test Error: 0.13220 (90000 epochs)\t\n",
      "Number of Parameters:  35775\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 2.75539\t Train Error: 0.90325\t Test Error: 0.89830 (0 epochs)\t\n",
      "Train MSE: 0.02547\t Train Error: 0.16550\t Test Error: 0.19980 (10000 epochs)\t\n",
      "Train MSE: 0.01417\t Train Error: 0.05750\t Test Error: 0.11120 (20000 epochs)\t\n",
      "Train MSE: 0.01214\t Train Error: 0.04675\t Test Error: 0.10960 (30000 epochs)\t\n",
      "Train MSE: 0.01086\t Train Error: 0.04225\t Test Error: 0.10990 (40000 epochs)\t\n",
      "Train MSE: 0.01003\t Train Error: 0.03800\t Test Error: 0.11260 (50000 epochs)\t\n",
      "Train MSE: 0.00951\t Train Error: 0.03475\t Test Error: 0.11790 (60000 epochs)\t\n",
      "Train MSE: 0.00884\t Train Error: 0.03275\t Test Error: 0.11910 (70000 epochs)\t\n",
      "Train MSE: 0.00848\t Train Error: 0.03125\t Test Error: 0.12250 (80000 epochs)\t\n",
      "Train MSE: 0.00806\t Train Error: 0.03075\t Test Error: 0.12410 (90000 epochs)\t\n",
      "Number of Parameters:  39750\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.75588\t Train Error: 0.90375\t Test Error: 0.90200 (0 epochs)\t\n",
      "Train MSE: 0.01638\t Train Error: 0.06900\t Test Error: 0.11060 (10000 epochs)\t\n",
      "Train MSE: 0.01336\t Train Error: 0.05275\t Test Error: 0.10710 (20000 epochs)\t\n",
      "Train MSE: 0.01181\t Train Error: 0.04250\t Test Error: 0.10920 (30000 epochs)\t\n",
      "Train MSE: 0.01077\t Train Error: 0.03600\t Test Error: 0.11180 (40000 epochs)\t\n",
      "Train MSE: 0.01000\t Train Error: 0.03225\t Test Error: 0.11480 (50000 epochs)\t\n",
      "Train MSE: 0.00938\t Train Error: 0.03025\t Test Error: 0.11670 (60000 epochs)\t\n",
      "Train MSE: 0.00891\t Train Error: 0.02800\t Test Error: 0.12100 (70000 epochs)\t\n",
      "Train MSE: 0.00855\t Train Error: 0.02575\t Test Error: 0.12320 (80000 epochs)\t\n",
      "Train MSE: 0.00818\t Train Error: 0.02550\t Test Error: 0.12490 (90000 epochs)\t\n",
      "Number of Parameters:  43725\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.71066\t Train Error: 0.89775\t Test Error: 0.89790 (0 epochs)\t\n",
      "Train MSE: 0.01713\t Train Error: 0.07850\t Test Error: 0.11970 (10000 epochs)\t\n",
      "Train MSE: 0.01404\t Train Error: 0.05850\t Test Error: 0.11570 (20000 epochs)\t\n",
      "Train MSE: 0.01253\t Train Error: 0.04925\t Test Error: 0.11270 (30000 epochs)\t\n",
      "Train MSE: 0.01150\t Train Error: 0.04300\t Test Error: 0.11550 (40000 epochs)\t\n",
      "Train MSE: 0.01071\t Train Error: 0.04025\t Test Error: 0.12020 (50000 epochs)\t\n",
      "Train MSE: 0.01007\t Train Error: 0.03875\t Test Error: 0.12360 (60000 epochs)\t\n",
      "Train MSE: 0.00954\t Train Error: 0.03400\t Test Error: 0.12990 (70000 epochs)\t\n",
      "Train MSE: 0.00914\t Train Error: 0.03275\t Test Error: 0.13290 (80000 epochs)\t\n",
      "Train MSE: 0.00881\t Train Error: 0.03175\t Test Error: 0.13610 (90000 epochs)\t\n",
      "Number of Parameters:  47700\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 2.27546\t Train Error: 0.88850\t Test Error: 0.91180 (0 epochs)\t\n",
      "Train MSE: 0.01640\t Train Error: 0.07025\t Test Error: 0.11340 (10000 epochs)\t\n",
      "Train MSE: 0.01356\t Train Error: 0.05250\t Test Error: 0.11120 (20000 epochs)\t\n",
      "Train MSE: 0.01198\t Train Error: 0.04225\t Test Error: 0.11520 (30000 epochs)\t\n",
      "Train MSE: 0.01082\t Train Error: 0.03675\t Test Error: 0.11690 (40000 epochs)\t\n",
      "Train MSE: 0.00995\t Train Error: 0.03050\t Test Error: 0.12040 (50000 epochs)\t\n",
      "Train MSE: 0.00919\t Train Error: 0.02625\t Test Error: 0.12550 (60000 epochs)\t\n",
      "Train MSE: 0.00858\t Train Error: 0.02450\t Test Error: 0.12830 (70000 epochs)\t\n",
      "Train MSE: 0.00811\t Train Error: 0.02375\t Test Error: 0.13210 (80000 epochs)\t\n",
      "Train MSE: 0.00773\t Train Error: 0.02300\t Test Error: 0.13660 (90000 epochs)\t\n",
      "Number of Parameters:  101760\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 2.57851\t Train Error: 0.92950\t Test Error: 0.89670 (0 epochs)\t\n",
      "Train MSE: 0.01407\t Train Error: 0.05100\t Test Error: 0.10090 (10000 epochs)\t\n",
      "Train MSE: 0.01077\t Train Error: 0.03725\t Test Error: 0.09560 (20000 epochs)\t\n",
      "Train MSE: 0.00903\t Train Error: 0.03000\t Test Error: 0.09510 (30000 epochs)\t\n",
      "Train MSE: 0.00790\t Train Error: 0.02325\t Test Error: 0.09710 (40000 epochs)\t\n",
      "Train MSE: 0.00707\t Train Error: 0.02100\t Test Error: 0.09930 (50000 epochs)\t\n",
      "Train MSE: 0.00640\t Train Error: 0.01900\t Test Error: 0.10180 (60000 epochs)\t\n",
      "Train MSE: 0.00589\t Train Error: 0.01725\t Test Error: 0.10410 (70000 epochs)\t\n",
      "Train MSE: 0.00546\t Train Error: 0.01675\t Test Error: 0.10600 (80000 epochs)\t\n",
      "Train MSE: 0.00509\t Train Error: 0.01625\t Test Error: 0.10910 (90000 epochs)\t\n",
      "Number of Parameters:  203520\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 1.86205\t Train Error: 0.93800\t Test Error: 0.90270 (0 epochs)\t\n",
      "Train MSE: 0.01325\t Train Error: 0.04225\t Test Error: 0.09200 (10000 epochs)\t\n",
      "Train MSE: 0.00969\t Train Error: 0.02625\t Test Error: 0.08470 (20000 epochs)\t\n",
      "Train MSE: 0.00780\t Train Error: 0.01450\t Test Error: 0.08030 (30000 epochs)\t\n",
      "Train MSE: 0.00653\t Train Error: 0.01000\t Test Error: 0.08160 (40000 epochs)\t\n",
      "Train MSE: 0.00557\t Train Error: 0.00775\t Test Error: 0.08290 (50000 epochs)\t\n",
      "Train MSE: 0.00484\t Train Error: 0.00625\t Test Error: 0.08210 (60000 epochs)\t\n",
      "Train MSE: 0.00426\t Train Error: 0.00475\t Test Error: 0.08230 (70000 epochs)\t\n",
      "Train MSE: 0.00380\t Train Error: 0.00425\t Test Error: 0.08560 (80000 epochs)\t\n",
      "Train MSE: 0.00342\t Train Error: 0.00425\t Test Error: 0.08580 (90000 epochs)\t\n",
      "Number of Parameters:  407040\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 2.33443\t Train Error: 0.90525\t Test Error: 0.90280 (0 epochs)\t\n",
      "Train MSE: 0.01367\t Train Error: 0.03575\t Test Error: 0.08690 (10000 epochs)\t\n",
      "Train MSE: 0.00950\t Train Error: 0.01650\t Test Error: 0.07960 (20000 epochs)\t\n",
      "Train MSE: 0.00740\t Train Error: 0.00925\t Test Error: 0.07460 (30000 epochs)\t\n",
      "Train MSE: 0.00603\t Train Error: 0.00600\t Test Error: 0.07270 (40000 epochs)\t\n",
      "Train MSE: 0.00506\t Train Error: 0.00475\t Test Error: 0.07220 (50000 epochs)\t\n",
      "Train MSE: 0.00431\t Train Error: 0.00325\t Test Error: 0.07220 (60000 epochs)\t\n",
      "Train MSE: 0.00373\t Train Error: 0.00250\t Test Error: 0.07140 (70000 epochs)\t\n",
      "Train MSE: 0.00327\t Train Error: 0.00200\t Test Error: 0.07190 (80000 epochs)\t\n",
      "Train MSE: 0.00289\t Train Error: 0.00200\t Test Error: 0.07290 (90000 epochs)\t\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from auto_tqdm import tqdm\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "SEED = 2134\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "def get_accuracy(preds, labels):\n",
    "    pred_max = torch.argmax(preds, dim=-1)\n",
    "    label_max = torch.argmax(labels, dim=-1)\n",
    "    return torch.sum(pred_max == label_max) / len(label_max)\n",
    "    \n",
    "train_set = train_set.double().cuda()\n",
    "test_set = test_set.double().cuda()\n",
    "train_labels = train_labels.double().cuda()\n",
    "test_labels = test_labels.double().cuda()\n",
    "\n",
    "widths = [16, 32, 40, 45, 50, 55, 60, 128, 256, 512]\n",
    "networks = []\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "test_losses = []\n",
    "test_accs = []\n",
    "num_params = []\n",
    "\n",
    "for width in widths:\n",
    "\n",
    "    net = Net(width)\n",
    "    num_parameters = 0\n",
    "\n",
    "    # Initialize the parameters i.i.d. from a standard normal \n",
    "    for idx, param in enumerate(net.parameters()):\n",
    "        init = torch.Tensor(param.size()).normal_()\n",
    "        param.data = init\n",
    "        num_parameters += param.view(-1).size()[0]\n",
    "    num_params.append(num_parameters)\n",
    "    net.double()\n",
    "    net.cuda()\n",
    "\n",
    "    print(\"Number of Parameters: \", num_parameters)\n",
    "    # Training neural network with GD\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=10)\n",
    "\n",
    "    epochs = 100000  \n",
    "    for i in tqdm(range(epochs), total=epochs):\n",
    "        net.zero_grad()\n",
    "        preds = net(train_set)\n",
    "        loss = torch.mean(torch.pow(train_labels - preds, 2))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss = loss.cpu().data.numpy()   \n",
    "        if i % 10000 == 0:\n",
    "            test_preds = net(test_set)\n",
    "            test_loss = torch.mean(torch.pow(test_labels - test_preds, 2))\n",
    "            train_acc = get_accuracy(preds, train_labels).cpu().data.numpy().item()\n",
    "            test_acc = get_accuracy(test_preds, test_labels).cpu().data.numpy().item() \n",
    "            print(\"Train MSE: %.5f\\t Train Error: %.5f\\t Test Error: %.5f (%d epochs)\\t\"%(loss, 1 - train_acc, 1 - test_acc, i))\n",
    "    networks.append(deepcopy(net))\n",
    "    train_losses.append(loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Infinite Width Limit of Neural Networks\n",
    "\n",
    "We now demonstrate the benefits of turning to the infinite width limit of a 1 hidden layer network given by the neural tangent kernel (NTK).  We will go over the construction of the NTK for a 1 hidden layer network in a separate notebook.  We emphasize the following takeaways of this notebook: \n",
    "\n",
    "1.  While training neural networks above is time consuming and requires a GPU, solving kernel regression below takes roughly 2 seconds on a CPU.  We also note that the code for solving kernel regression is far more concise and simpler than that for training neural networks of varying width. \n",
    "\n",
    "2.  For this case of fully connected networks, solving kernel regression with the NTK gives a model that far outperforms the corresponding finite width neural network.  We note this is not always the case for general network architectures (see https://arxiv.org/abs/2007.15801), but given the simplicity of solving kernel regression, the NTK gives a strong, simple baseline. \n",
    "\n",
    "3.  Interpolation is not at odds with generalization.  Solving kernel regression exactly with the NTK gives us a training MSE of around $10^{-7}$, but gives us remarkably high test accuracy (around 96%).  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train MSE: 0.00000\t Train Error: 0.00000\t Test Error: 0.04390 \t\n",
      "Time taken:  2.1329290866851807\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import pinv, solve\n",
    "import time \n",
    "\n",
    "def mse(preds, labels): \n",
    "    return np.mean(np.abs(np.power(preds - labels, 2)))\n",
    "\n",
    "def numpy_acc(preds, labels):\n",
    "    preds_max = np.argmax(preds, axis=0)\n",
    "    labels_max = np.argmax(labels, axis=0)\n",
    "    return np.mean(preds_max == labels_max)\n",
    "\n",
    "# Infinite Width Random ReLU Feature Regression\n",
    "def ntk(pair1, pair2):\n",
    "\n",
    "    out = pair1 @ pair2.transpose(1, 0) + 1\n",
    "    N1 = np.sum(np.power(pair1, 2), axis=-1).reshape(-1, 1) + 1\n",
    "    N2 = np.sum(np.power(pair2, 2), axis=-1).reshape(-1, 1) + 1\n",
    "\n",
    "    XX = np.sqrt(N1 @ N2.transpose(1, 0))\n",
    "    out = out / XX\n",
    "\n",
    "    out = np.clip(out, a_min=-1, a_max=1)\n",
    "\n",
    "    first = 1/np.pi * (out * (np.pi - np.arccos(out)) \\\n",
    "                           + np.sqrt(1. - np.power(out, 2))) * XX\n",
    "    sec = 1/np.pi * out * (np.pi - np.arccos(out))\n",
    "    out = first + sec\n",
    "    return out \n",
    "\n",
    "X = train_set.cpu().data.numpy().astype(\"float32\")\n",
    "y = train_labels.cpu().data.numpy().astype(\"float32\")\n",
    "X_test = test_set.cpu().data.numpy().astype(\"float32\")\n",
    "y_test = test_labels.cpu().data.numpy().astype(\"float32\")\n",
    "\n",
    "start = time.time()\n",
    "# Build kernel matrix for train & test data\n",
    "K_train = ntk(X, X) # + np.eye(len(K_train)) * .05\n",
    "K_test = ntk(X, X_test)\n",
    "\n",
    "# Solve kernel regression\n",
    "a_hat = solve(K_train, y).T\n",
    "end = time.time()\n",
    "\n",
    "# Get error on train & test data\n",
    "train_error = mse(a_hat @ K_train, y.T)\n",
    "test_error =mse(a_hat @ K_test, y_test.T)\n",
    "inf_train_acc = numpy_acc(a_hat @ K_train, y.T)\n",
    "inf_test_acc = numpy_acc(a_hat @ K_test, y_test.T)\n",
    "\n",
    "print(\"Train MSE: %.5f\\t Train Error: %.5f\\t Test Error: %.5f \\t\"%(train_error, 1 - inf_train_acc, 1 - inf_test_acc))\n",
    "print(\"Time taken: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f65ad8f5310>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEaCAYAAAAL7cBuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAA33klEQVR4nO3dd5xU5b3H8c+XBSnSFFCjVJUitpXsohQRY4WoaCSWoAZLgNgu15JrS2KiWGKJN0ajWIJRLERjx4gaEQWVoouICqKCrOilKAqCCvK7fzxnZHZ3Zmd22dkzu/t7v17nNTOn/mZ2dn7neZ5znkdmhnPOOVdeo7gDcM45l588QTjnnEvJE4RzzrmUPEE455xLyROEc865lDxBOOecS8kTRB0iaaqkM9Is6yrJJDWu7bjqMkmXSLqzkuUjJb1SmzE1JP69zW+eIGqQpMWS1ktaI2m1pBmSxkjK689Z0uWSNkRxr5G0UNJfJf0o7tjSiX5Udt3S/ZjZVWZ2RrTPLf6xSvoOrJX0maQJklpmuW3KE4BonweXm5c2cUkaHL2PW8rNf0XSyCxjqZHPN19F33mTdG65+WOj+ZdHrzN+luX/FpIGRv/7X0r6XNJ0ScXRycjaaPpG0vdJr+fn/l1XXV7/cNVRR5pZK6ALcA3wP8Bd8YaUlYeiuLcFjgF2AObkc5LIY0eaWUugENgHuDiGGL4GTpHUNYZjZyUPSg0LgV+Wm3dKND9Z1p+lpNbAU8DNhP+lnYA/AN9GJyMto+/GGODVxGsz233L3kpueILIETP70syeAI4HfilpDwBJbST9Q9IKSUskXZYoYURnNfcl9pHmjHYXSTOjs5PHJW2b6vjRce6S9KmkTyRdKakgi7g3mNn8KO4VwPlJ+zxCUklS6WivpGX/Ex1njaQFkg6K5hdEZ04fRMvmSOoULesl6bnoLGuBpOOS9jdB0i2Sno62e13SLtGyadFqc6Ozr+NTvP8lkn4cPT8p+hx7R6/PkPRYis88sd/V0X77Je3veklfSPpI0pBMn2P0WX4GPEtIFIn97Bd9dqslzZU0OJt9VcNqYALw+3QrSDpN0rvR+3pWUpdofoXPV9JLko6Nlg+MPs+h0euDJZVEzxtF3+klkpZH3/U20bLE9/l0SR8D/0kR07EKJaY90sT8K0mLou/ME5J2TFpmCiX296P3dIskVfIZzQJaSNo92n53oHk0v0qfZZIeAGb2gJl9b2brzWyKmb2VxbZ5xxNEjpnZTKAU2D+adTPQBtgZOIBwxnJqFXZ5CnAasCOwEfhLmvXuiZbvSjiLPRRI2X6RJu7vgccTcUvqA9wNjAbaAbcDT0hqKqkncDZQHJVCDgMWR7s6DzgRGAq0jmJfJ2lr4DngfmC7aJ1bE/+skRMJZ1/bAIuAcVFsg6Lle0dnXw+leAsvAYOj54OADwmfd+L1Sym2Sey3bbTfV6PX+wILgPbAn4C7MvzwACCpIzAkih1JOwFPA1cSzi4vAB6R1CHTvqppHHBs9PcpH9vRwCXAz4AOwMvAA5D288328xwZTQcSvuMtgb+WO/wBwG6E70lyTKcC1wIHm9nbKWL+CXA1cBzwI2AJ8GC51Y4AioG9o/UOo3L3Ev6nIJQm/pFmvbSfZTkLge8l3SNpiKRtMqyf1zxB1I5lwLbRGfzxwMVmtsbMFgM3ACdXYV/3mtnbZvY18FvguPIlA0nbE36YxprZ12a2HPgzcEJ14o6e/wq43cxej86M7gG+BfYDvgeaAr0lNTGzxWb2QbTdGcBlZrbAgrlmtorwj7zYzP5uZhvN7A3gEWB40vH/ZWYzzWwjMJGkM/EsvMTmH7D9CT8sidcHkDpBpLPEzO6IkuY9hB+n7StZ/zFJa4ClwHI2n3meBEw2s8lmtsnMngNmE5JnjYtKMLcBf0yxeDRwtZm9G32+VwGFiVJECsmf5yDSf54jgBvN7EMzW0uoXjtBZUvBl0ffy/VJ88YCFwKDzWxRmhhGAHeb2Rtm9m20734qW/VzjZmtNrOPgRfJ/J25DzhRUhPC/8d9qVbK8Fkmr/cVMBAw4A5gRVTSqez7krc8QdSOnYDPCWegWxHOfBKWRMuztbTctk2i/SbrEs3/NKrKWE0449+uamH/EHdin+cn9hftsxOwY/QPPRa4HFgu6cGkon8n4AMq6gLsW25/IwhtHwmfJT1fRzgbzdZLwP6SdgAKgIeAAdGPSRugpAr7+iEOM1sXPa0slqOjktRgoBeb/z5dgJ+Xe88DCQmnMhsJf89kTYANWcR+LXCYpL3Lze8C/G9SHJ8DIv138VWgR/RDV0g40+4kqT3Ql83VcztS8fvdmLIJNfk7nHAhcIuZlVbyXsrsO0pAq8rFXKXvTJRIFhES5Ptmliq2hHSfZfl9vmtmI82sI7BHFPdNlW2TrzxB5JikYsIX+BVgJeGfOvksrTPwSfT8a6BF0rLkH8uETuW23RDtN9lSwtl9ezNrG02tq9IQptAuciSh6iGxz3FJ+2trZi3MLFEtcb+ZDYzemxH+mRLb7ZLiEEuBl8rtr6WZ/TrbGCsTJa11wLnANDNbQ/jxGAW8YmabUm1WE8dOiuElQt319dGspYQSYPJ73trMrsmwq4+BruXmdaPsD3G6GFYRfpyuKLdoKTC6XCzNzWxGmv2sA+YA/wW8bWbfATMIVYgfmFniO7iMit/vjcD/Je8uxSEOBS5LtHOkUWbfUTVlOzb//1TXPwhtbemql4BKP8vKtnmP8B1I2aaS7zxB5Iik1pKOINSR3mdm86IqiknAOEmtouL8eWwu1pYAgyR1jhr2Ul39cpKk3pJaEIq7D0f7/YGZfQpMAW6I4mgkaRdJB6TYX/m4m0jajVAfvQNwY7ToDmCMpH0VbC3pp9H76CnpJ5KaAt8A6wnVTgB3AldI6h5tt5ekdoQrPXpIOjk6ZhOFSwF3y+LjhfCDs3OGdV4itI0kqj+mlntd3gpgUxb7rYqbgEMkFRL+zkdKOkyh8b6ZwmWUHZPWbxzNT0xNCKWfsQqN+pJURGjLKV//ns6NQH9CvX/CbcDFSQ20bST9PGl5qs83m8/zAeC/JXVTuLz3KsIVchszxDgfOBy4RdJRada5HzhVUmH0XbsKeD2qqt0SDxES1KQs1k31Wf4g+hudn/ibKlyQcSLw2hbGGAtPEDXvyaT650sJX6jkRuhzCCWFDwmlivsJjb9EddIPAW8RztaeSrH/ewlnJJ8BzQhnyKmcQqjOegf4AniYyqsyjpe0lnDFxhOEovuPzWxZFNtsQjvEX6P9LSI0RkJof7iGUJL5jFCVdUm07EbCP94U4CvCJb/NozP6Qwn1vsui7a6N9pWNy4F7oiqS49Ks8xLQis3VH+VflxGdJY8Dpkf73S/LWNIysxWEM9PfRtUXwwifzQrCd+RCyv4f/o2QYBPT3wnJ+e/Ak8CX0f4uNbN/ZxnDV4TG9W2T5j1K+LwflPQV8Dah3Srhcip+vtl8nncTvqPTgI8IJwznZBnnXELb1B1KcaWYmb1AaHd7BPiUUDKtartaquOuN7Pny7WJpFu3wmdZzhrCRQ2vS/qakBjeJulqwLpE5gMGOeecS8FLEM4551LyBOGccy4lTxDOOedS8gThnHMuJU8QzjnnUoq7N8Ua1b59e+vatWvcYbgttWBBeOyZqdsb59yWmjNnzkozS9kfWL1KEF27dmX27Nlxh+G21ODB4XHq1DijcK5BkJT2jvx6lSBcPXHZZXFH4JzDE4TLRwcfnHkd51zOeSO1yz8lJWFyzsXKSxAu/4wdGx69DaJO2rBhA6WlpXzzzTdxh+KSNGvWjI4dO9KkSfme49Nr8Ali4kS49FL4+GPo3BnGjYMRI+KOyrm6q7S0lFatWtG1a1eUeeA9VwvMjFWrVlFaWkq3bt2y3q5BVzFNnAijRsGSJWAWHkeNCvOdc9XzzTff0K5dO08OeUQS7dq1q3KprkEniEsvhXXrys5bty7Md85VnyeH/FOdv0mDThAff1y1+c65uqFly8yj07788svsvvvuFBYW8sknnzB8+PCM2wwdOpTVq1ezevVqbr311irF9L//+7+MTbSvAaNHj+bgpCv2br75Zs4991xmz57NueemHuala9eurFy5ssLxp06dyhFHHFGleLLRoBNE585Vm+9qyVVXhcm5HJo4cSIXXHABJSUl7LTTTjz88MMZt5k8eTJt27atVoLo378/M2ZsHtG1pKSEL7/8ku+/D4MvzpgxgwEDBlBUVMRf/vKXSvdVneNXR4NOEOPGQYsWFecXF4c2CReT/v3D5NwWmjp1KoMHD2b48OH06tWLESNGYGbceeedTJo0iT/+8Y+MGDGCxYsXs8ceYdjoCRMm8LOf/YzDDz+c7t2785vf/OaH/SXO4C+66CI++OADCgsLufDCCwG47rrrKC4uZq+99uL3v/99hVj22WcfFi5cyPr16/nyyy9p0aIFhYWFzJs3DwgJon///mVKA6tWreLQQw9ln332YfTo0SQGeEt1/LVr11Z4n1uqQV/FlLhaKXEVU8eOofTw8MNw/PHw97/D1lvHG2ODlDjL8iRRLwxOdJ2S5LjjjuPMM89k3bp1DB06tMLykSNHMnLkSFauXFmh6mdqFS9/fvPNN5k/fz477rgjAwYMYPr06Zxxxhm88sorHHHEEQwfPpzFixeX2aakpIQ333yTpk2b0rNnT8455xw6der0w/JrrrmGt99+m5Lofp0pU6bw/vvvM3PmTMyMo446imnTpjFo0KAftmncuDGFhYXMmjWL9evXs++++9K9e3dmzJjBdttth5nRqVMnPvjggx+2+cMf/sDAgQP53e9+x9NPP8348eNTHn/q1Kkp3+fAgQOr9FmV16BLEBCSxOLFsGlTSBIvvwzXXReSxP77e3tELC65JEzO1YC+ffvSsWNHGjVqRGFhYYVkkMpBBx1EmzZtaNasGb1792bJkrTdFQEhQUyZMoV99tmHPn368N577/H+++9XWG/AgAHMmDGDGTNm0K9fP/r168eMGTOYPn06/VOcEE2bNo2TTjoJgJ/+9Kdss802Nfo+M2nQJYhUJLjgAujdG048MVQ3/etfMGBA3JE5VzdVdsbfokWLSpe3b9++yiWG8po2bfrD84KCAjZu3Fjj25gZF198MaNHj650vf79+3P77bfzzTffcNZZZ9GhQwfeeecdOnTowIA0PzLZXn1UnfeZSYMvQaQzdCi8/jq0aQMHHgh33x13RM65fNGqVSvWrFnzw+vDDjuMu+++m7Vr1wLwySefsHz58grb9e/fn9dee40VK1aw3XbbIYkOHTrw+OOPpyxBDBo0iInRjVnPPPMMX3zxRcrj54oniEr06hWSxODBcPrp8F//BTWQlJ1zdVy7du0YMGAAe+yxBxdeeCGHHnoov/jFL+jXrx977rknw4cPT/kDvs0229ChQwd23333H+b169eP5cuXs/fee1dY//e//z3Tpk2jT58+TJkyhc7RJZblj58rqomW7nxRVFRkuRgPYuNG+M1v4M9/Dh2NPvQQbLttjR/GJfh4EHXau+++y2677RZ3GC6FVH8bSXPMrCjV+jktQUg6XNICSYskXZRi+TBJb0kqkTRb0sCkZYslzUssy2WcmTRuDDfeGKqZpk2Dvn3hnXfijKieu+mmMDnnYpWzBCGpALgFGAL0Bk6U1Lvcai8Ae5tZIXAacGe55QeaWWG67FbbTj0VXnwR1q6F/faDp56KO6J6qrAwTM65WOWyBNEXWGRmH5rZd8CDwLDkFcxsrW2u49oayPv6rv79YdYs6N4djjoKrr3Wb6qrcc8/HybnXKxymSB2ApYmvS6N5pUh6RhJ7wFPE0oRCQZMkTRH0qgcxlllnTqF+yWOOw4uughOOgnWr487qnrkyivD5Oqs+tS2WV9U52+SywSR6uLdChGa2aNm1gs4GrgiadEAM+tDqKI6S9Kg8tsCSBoVtV/MXrFiRQ2EnZ0WLeCBB0J3HfffD4MGwSef1NrhnctbzZo1Y9WqVZ4k8khiPIhmzZpVabtc3ihXCnRKet0RWJZuZTObJmkXSe3NbKWZLYvmL5f0KKHKalqK7cYD4yFcxVSTbyATKdzwu/vuoRRRXAyPPgr77lubUTiXXzp27EhpaSm1ecLmMkuMKFcVuUwQs4DukroBnwAnAL9IXkHSrsAHZmaS+gBbAaskbQ00MrM10fNDgT/mMNYtMmwYvPpqaJM44AC44w44+eS4o3IuHk2aNKnSqGUuf+UsQZjZRklnA88CBcDdZjZf0pho+W3AscApkjYA64Hjo2SxPfBodIt5Y+B+M/t3rmKtCXvsERqvf/5zOOUUmDcPrr4aCgrijsw556rHb5SrYRs2wH//N9xyCwwZEtop2rSJNaS6Z8GC8NizZ7xxONcAxHajXEPUpAn89a9w223w3HOhPWLhwrijqmN69vTk4Fwe8ASRI6NHwwsvwKpVIUlMmRJ3RHXIk0+GyTkXK08QOTRoUGiX6Nw5VDf9+c9+U11WbrghTM65WHmCyLGuXWH69HCl03nnwWmnwbffxh2Vc85l5gmiFrRsGUao+/3vYcKEML7EZ5/FHZVzzlXOE0QtadQILr8c/vlPmDs33FQ3Z07cUTnnXHqeIGrZ8OGhyqlRIxg4EB58MO6InHMuNR+TOgaFhaHx+thjw7jX8+bBFVeEpOGAe++NOwLnHF6CiM1224XLYM84A666Co45BmphiNm6oVOnMDnnYuUJIkZbbQXjx8PNN8PTT0O/fvDhh3FHlQceeihMzrlYeYKImQRnnw3PPgvLloXG6xdfjDuqmP3tb2FyzsXKE0SeOOig0C6xww5wyCFw661+U51zLl6eIPLILruEbsOHDIGzzoJf/xq++y7uqJxzDZUniDzTujU89hhcfDHcfnsoTfi4K865OHiCyEMFBeHKpvvvh5kzQ7vE3LlxR+Wca2g8QeSxE0+El1+GjRuhf3945JG4I6olDz8cJudcrDxB5LmiotB4veee4S7sP/wBNm2KO6oca98+TM65WHmCqAN+9COYOjUMZXr55XDccfD113FHlUMTJoTJORcrTxB1RLNm4Tfzhhvg0UdhwABYsiTuqHLEE4RzecETRB0ihTElnn4aFi8Ojdcvvxx3VM65+soTRB10+OHw+uuwzTbhBrs77og7IudcfeQJoo7q2TMkiZ/8BEaNgnPOgQ0b4o7KOVef5DRBSDpc0gJJiyRdlGL5MElvSSqRNFvSwGy3ddC2bahuOv98+OtfQ8li1aq4o3LO1ReyHHX4I6kAWAgcApQCs4ATzeydpHVaAl+bmUnaC5hkZr2y2TaVoqIimz17dk7eT767555QkujUCR5/HHbfPe6ItsC6deGxRYt443CuAZA0x8yKUi3LZQmiL7DIzD40s++AB4FhySuY2VrbnKG2BizbbV1Zv/wlvPRSuPy1Xz948sm4I9oCLVp4cnAuD+QyQewELE16XRrNK0PSMZLeA54GTqvKttH2o6LqqdkrGninRfvtF26q69EDhg2Da66poz3C3nprmJxzscplglCKeRV+rszsUTPrBRwNXFGVbaPtx5tZkZkVdejQobqx1hsdO4ZLX084IXT4N2IErF8fd1RVNGlSmJxzscplgigFkseN7AgsS7eymU0DdpHUvqrburKaN4eJE+Hqq+HBB2H//aG0NO6onHN1TS4TxCygu6RukrYCTgCeSF5B0q6SFD3vA2wFrMpmW1c5CS66KDRYL1gQ+nR69dW4o3LO1SU5SxBmthE4G3gWeJdwhdJ8SWMkjYlWOxZ4W1IJcAtwvAUpt81VrPXZkUfCa6/B1lvD4MHhaqeJE6FrV2jUKDxOnBhzkM65vJSzy1zj0JAvc81k1arQyd9//gONG4cuxBNatIDx40N7RV4YPDg8Tp0aZxTONQiVXebqCaIB2bAB2rWDNWsqLuvSJfTv5JxrWOK6D8LlmSZNYO3a1Ms+/rh2Y3HO5T9PEA1M585Vmx+L668Pk3MuVp4gGphx4yrepNyiRZifN556KkzOuVhlnSAkbSNpd0k7S/LEUkeNGBEapNu2Da87dsyzBmrnXN5oXNlCSW2As4ATCfcorACaAdtLeg241cxezHmUrkaNGBESw+DBITkMGRJ3RM65fJSpJPAwoU+k/c2sp5kNjLq16ARcAwyTdHrOo3Q1rk+fcDPdzJlxR+Kcy1eVliDM7JBKls0B5tR4RK5WtGoFu+0WOvfLO82bxx2Bc44MCcLVb8XF8MwzocdXpeoeMS7PPBN3BM45tuAqJklv1GQgrvYVF8Py5bB0aeZ1nXMNT7UThJn1qclAXO3r2zc85l07xBVXhMk5F6usEkTUq2qzpNfNJXXNWVSuVuy1V7i7Ou/aIV54IUzOuVhlW4L4J7Ap6fX30TxXhzVtCnvvnYcJwjmXF7JNEI2jsaEBiJ5vlZuQXG0qLoY5c2DTpszrOucalmwTxApJRyVeSBoGrMxNSK42FRfDV1/BwoVxR+KcyzfZXuY6Bpgo6a/R61LglNyE5GpTckN1r17xxvKDdu3ijsA5R5YJwsw+APaT1JIwhkSKEQVcXdSrVxhtbtYsOCVfUv4jj8QdgXOO7K9iukpSWzNba2Zroo77rsx1cC73Cgrgxz/2hmrnXEXZtkEMMbPViRdm9gUwNCcRuVpXXAwlJfDddxlXrR0XXxwm51yssm2DKJDU1My+hXAfBNA0d2G52lRcDN9+C2+/HTrxi92rr8YdgXOO7EsQ9wEvSDpd0mnAc8A/cheWq015e0e1cy5WWSUIM/sTcCWwG7A7cIWZXZtpO0mHS1ogaZGki1IsHyHprWiaIWnvpGWLJc2TVCJpdvZvyVVV167hwiFvh3DOJcu6N1cz+zfwb0lbA8dIetrMfppufUkFwC3AIYTLYmdJesLM3kla7SPgADP7QtIQYDywb9LyA83M77fIMSlUM3mCcM4ly/Yqpq0kHS1pEvApcBBwW4bN+gKLzOzD6M7rB4FhySuY2YyowRvgNaBjlaJ3Naa4GObPh6+/jjsSwnB3Hf2r4FzcMg05eghhuNHDgBeBe4G+ZnZqFvveiTAaXUIpZUsH5Z0OJA8EYMAUSQbcbmbj08Q4ChgF0Llz5yzCcqkUF4fuNt58EwYOjDmY++6LOQDnHGQuQTwL7AIMNLOTzOxJynbaV5lUQ9BYyhWlAwkJ4n+SZg+IuhQfApwlaVCqbc1sfDQMalGHDh2yDM2VV1wcHr2h2jmXkClB/JhQ9fO8pOei8acLstx3KdAp6XVHYFn5lSTtBdwJDDOzVYn5ZrYselwOPEqosnI5ssMO0KlTnrRDjB0bJudcrCpNEGb2ppn9j5ntAlwO7ANsJemZqGqnMrOA7tFYElsBJwBPJK8gqTPwL+BkM1uYNH9rSa0Sz4FDgber9tZcVeVNQ3VJSZicc7HKekQ5M5tuZmcT2hZuAvplWH8jcDahmupdYJKZzZc0RtKYaLXfAe2AW8tdzro98IqkucBM4OnoKiqXQ8XF8MEH8PnncUfinMsHWV/mmmBmmwg/+s9mse5kYHK5ebclPT8DOCPFdh8Ce5ef73Ir0Q4xezYcemi8sTjn4lftMald/VNUFB69odo5B9UoQbj6q00b6NkzD9ohevSIOQDnHFQjQUgale6eBFf3FRfDCy/EHMR4/3o5lw+qU8U0JvMqrq4qLoZPP4VPPok7Eudc3KqTIFLdAOfqiby4YW7UqDA552JVnQRxZI1H4fJGYSE0bhxzO8TChWFyzsWqygnCzEpzEYjLD82bw5575kFDtXMudn6Zq6uguDjcC2Epe85yzjUUniBcBcXFsHo1LFoUdyTOuThVO0FEXYG7eij2hurCwjA552K1JTfK3QX4AAz10O67h7aIWbNgxIgYArjpphgO6pwrL9OAQU+kW0ToZM/VQ40bQ58+3lDtXEOXqQSxP3ASsLbcfOHjM9RrxcVw++2wcWNIGLXqpJPCo48s51ysMv3rvwasM7OXyi+QtCA3Ibl8UFwcanrmz4e9a7tf3VK/ktq5fJBpwKAhZvZimmUphwB19UPsDdXOudhVmiAkZexWI5t1XN2z667Qtq23QzjXkGW6zPVFSedEQ4P+QNJWkn4i6R7gl7kLz8VFyqMhSJ1zscjUBnE4cBrwgKRuwGqgGVAATAH+bGYluQzQxae4GK69FtavD5e91pp+lY5m65yrJZUmCDP7BriVMGZ0E6A9sN7MVtdCbC5mxcXw/fdQUlLLv9lXX12LB3POpZP1ndRmtsHMPvXk0HB4Q7VzDZv3xeTS2mkn2HHHGNohjj02TM65WOU0QUg6XNICSYskXZRi+QhJb0XTDEl7Z7utqx2xNFSvWhUm51ysMiYISQWSnq/qjiUVALcAQ4DewImSepdb7SPgADPbC7gCGF+FbV0tKC4OY/esXh13JM652pYxQZjZ98A6SW2quO++wCIz+9DMvgMeBIaV2/cMM/sievka0DHbbV3tSLRDzJkTbxzOudqXbS873wDzJD0HfJ2YaWbnVrLNTsDSpNelwL6VrH868Ew1t3U5UlQUHmfOhIMOijcW51ztyjZBPB1NVZHqDuuUY5RJOpCQIAZWY9tRwCiAzp299/Gatu224a7qWm2H8EzkXF7IKkGY2T2StgJ6RLMWmNmGDJuVAp2SXncElpVfSdJewJ3AEDNbVZVto9jGE7VdFBUV+SCZOVBcDC+/XIsH/O1va/Fgzrl0srqKSdJg4H1Cw/GtwEJJmTrrmwV0l9QtSi4nAGXGl4i68PgXcLKZLazKtq72FBeHDlY/+yzuSJxztSnbKqYbgEPNbAGApB7AA8CP021gZhslnQ08S+ia424zmy9pTLT8NuB3hIGHbo36/NtoZkXptq3WO3RbLNFQPWsWHHlkLRxwyJDw+Mwzla/nnMupbBNEk0RyADCzhVHXG5Uys8nA5HLzbkt6fgZwRrbbunjssw8UFISG6lpJEOvX18JBnHOZZJsg5ki6C7g3ej0C8AsfG4ittw7jVHvPrs41LNneST0GmA+cC/wX8E40zzUQiTuqzS8DcK7ByFiCkNQImGNmewA35j4kl4+Ki+Guu+Cjj2DnneOOxjlXGzImCDPbJGmupM5m9nFtBOXyT3LPrjlPEEcckeMDOOeykW0bxI+A+ZJmUvZO6qNyEpXLO3vuCU2bhmqmE07I8cEuuCDHB3DOZSPbBPGHnEbh8l6TJuFqJm+odq7hyLYN4paoDcI1YMXFcPfdYZS5goIcHmjw4PA4dWoOD+KcyySb3lw3AXOju55dA1ZcDF9/De++G3ckzrna4G0QLmvJDdV7eHnSuXrP2yBc1nr0gNatQzvEaafFHY1zLtcqTRCSepnZe2b2kqSmZvZt0rL9ch+eyyeNGoXxIbyh2rmGIVMbxP1Jz18tt+zWGo7F1QHFxfDWW/Dtt5nXrbbjjguTcy5WmaqYlOZ5qteuASguhg0bYO5c6Ns3Rwc588wc7dg5VxWZShCW5nmq164BSG6ozpl168LknItVphJER0l/IZQWEs+JXu+U08hcXurUCbbfPsftEEOHhke/D8K5WGVKEBcmPZ9dbln5164BkDb37Oqcq98qTRBmdk9tBeLqjuJiePppWLMGWrWKOxrnXK5kOx6Ecz8oLg7jQszxIaOcq9c8Qbgqq5WGaudc7LK6k1rSADObnmmeaxjat4du3XLYDjFyZI527Jyrimy72rgZ6JPFPNdAFBfD66/naOeeIJzLC5m62ugH9Ac6SDovaVFrIJcdPrs8V1wMkybBihXQoUMN73zlyvDYvn0N79g5VxWZ2iC2AloSEkmrpOkrYHimnUs6XNICSYskXZRieS9Jr0r6VtIF5ZYtljRPUokkv6Q2zyTaIXJSzTR8eJicc7HKdJnrS8BLkiaY2RL4YQChlmb2VWXbSioAbgEOAUqBWZKeMLN3klb7HDgXODrNbg40s5VZvRNXq/r0CfdEzJy5+b4251z9ku1VTFdLai1pa+AdYIGkCzNs0xdYZGYfmtl3wIPAsOQVzGy5mc0CNlQ1cBevVq2gd2+/Yc65+izbBNE7KjEcDUwGOgMnZ9hmJ2Bp0utSqtY9hwFTJM2RNCrdSpJGSZotafaKFSuqsHu3pRJ3VJv3yuVcvZRtgmgiqQkhQTxuZhvI3Flfqt5eq/JTMsDM+gBDgLMkDUq1kpmNN7MiMyvqUOOtpa4yxcWhkfrjj+OOxDmXC9le5no7sBiYC0yT1IXQUF2ZUqBT0uuOwLJsAzOzZdHjckmPEqqspmW7vcu95BvmunSpwR3/+tc1uDPnXHVlVYIws7+Y2U5mNtSCJcCBGTabBXSX1E3SVsAJwBPZHE/S1pJaJZ4DhwJvZ7Otqz177QVNmuSgHeL448PknItVtndSbw9cBexoZkMk9Qb6AXel28bMNko6G3iWcM/E3WY2X9KYaPltknYg9ArbGtgkaSzQG2gPPCopEeP9Zvbvar5HlyNNm0JhYQ4SxNKo6apTp8rXc87lVLZtEBMIP/Q7Rq8XAmMzbWRmk82sh5ntYmbjonm3mdlt0fPPzKyjmbU2s7bR86+iK5/2jqbdE9u6/FNcHDrt27SpBnd68slhSmHiROjaNYyP3bVreF2V5c657FWaICQlShjtzWwSsAlC6QD4PsexuTqguDh0+71gQe6PNXEijBoFS5aEK6eWLAmvE0kg03LnXNVkKkEk+uv8WlI7oquQJO0HfJnLwFzdUJs9u156acWRSNetC23aZ5wRkkGq5ZdemvvYnKuPMiWIxKWq5xEamHeRNB34B3BOLgNzdcMbb4Q7qkeOzH2VTrrLadesgWeeST+M9ZIlIYHcdx+UluYuPufqm0yN1Mmd9D1KuElOwLfAwcBbOYzN5bmJE2HMmM03yiWqdABGjKjZY5lB27bwxRcVl3XpAosXhwS1ZEnF5c2bwyOPwF3RJRW77gqDB4fpgAOgY8eajdW5+iJTCaKA0FlfK2BrQkIpAFpE81wDlq7KZ4urdM4/P0yR9evhlFNCcigo14dwixYwLrqEYdy48Lr88jvuCB3EvvEG3Hhj6CLk4YfhpJPChVLdu8OvfhUSnpcwnNtMVkk/CZLeiO5mrhOKiops9mzv+LW2NGqUvpuNd96B3Xbb8mOUlsIxx8Ds2fDHP4aBii67LFQ3de4ckkJyaWXixJCg0i1P+P57eOstmDo1TC+9BF9GrWrJJYzBg2GnqnQQ41wdI2mOmRWlXJYhQbxpZvvkLLIa5gmidqWr0lHUcnXMMXDxxVCU8quX2sSJcNdvFrBsGazevifr14ckdN99cNRRNRJ2Sp4wXEO1JQliWzP7PGeR1TBPELUrcVlpcjVTixZwww3wySdw883hR/bgg+GSS2DZssrP7hP7e3rdYAAOZCoSXHMN/OY3tfvePGG4hqLaCaKu8QRR+yqr0vnqK7jttlDv/3//F6qkkm+o22qr0Lawxx5h3euvD48vMhgICQI2N0LH6fvvYe7czQlj2jRPGK5+8AThYvXNN+FH8/Msy6LlE4RUw3dq14DKEkb37mWvkvKE4fJZZQki295cnau2Zs1SX54K4cd/5cowAFH37qnbNDp3zm181VFQEEbV69MHzjuvYsKYNClcPQWeMFzdlW1fTM5tkXQ/8p07w7bbhl5h012mmriMNZ8lEsZ558ETT8CqVaGPqhtugF69QsIYMSLcc9GjR2hruf/+0FbjXL7yKiZXK9I1aI8fX7GhevJ5z7N8Obzf5eC0l6nWNdlWSQ0eDDvumHY3ztU4b4NweSHbexQaAk8YLl94gnB1S0lJeCwsjDOKWlVZwujRo2wbhicMV5M8Qbi6ZfDg8Dh1apxRxMoThqstlSUIb6R2Lg9V1ujdsyc89BD84hfhiqiePWH0aHjggXAzIvjASa5meAnC5R8vQWRUWQljhx3CpcMbN25eP9UFAc6BlyCcq3dSlTBmzw53o3/5ZdnkAOHqsXPPhbffzr+bDl3+8gThXD1QUAA//nHoJf2bb1Kv8/nnsOee0K4d/PSncNVVoeSxfn3txurqDr+T2uWfq66KO4I6rXPn1Hek77hj+GinT4dXXoHJk8P8Jk1CaWTAABg4MDxut13txuzyU05LEJIOl7RA0iJJF6VY3kvSq5K+lXRBVbZ19Vj//mFy1ZLujvQ//Ql++cvQFvHOO6Gd4sknQzVVkyZwyy3ws5/B9tuHezFOPRXuvBPefTf9uB+ufstZI7WkAmAhcAhQCswCTjSzd5LW2Q7oAhwNfGFm12e7bSreSF1PzJgRHj1JVFt1bkr89ttwpdT06ZunlSvDsm23DSWLxFRUFPrYcnVfLPdBSOoHXG5mh0WvLwYws6tTrHs5sDYpQWS9bTJPEPWEX8WUF8xg4cLNyeKVV8JrCF21FxVtrpbq3x/at483Xlc9cfXmuhOwNOl1KbBvTW8raRQwCqBzPnb76VwdJYV7LHr2hNNOC/NWrAgFvFdeCUnjppvguuvCsp49y7ZjdO++eXRBVzflMkGk+mpkW1zJelszGw+Mh1CCyHL/zrlq6NABhg0LE4QroGbP3lzKeOwxuPvuzesmV0v16QNNm8YWuquGXCaIUqBT0uuOwLJa2NY5V0uaN4f99w8ThHss3nuvbLXUY4+FZU2bQt++mxNG//6hbcPlr1wmiFlAd0ndgE+AE4Bf1MK2zrmYNGoEvXuH6Ve/CvM++6xstdT114dxxiGsl1wttfPOXi2VT3La1YakocBNQAFwt5mNkzQGwMxuk7QDMBtoDWwC1gK9zeyrVNtmOp43UtcTDbA314Zk3TqYOXNzKWPGjM3dhGy/fdmEsc8+4RJclzvem6tzLm9t2gTz52+ukpo+HRYvDsuaN4d9991cLdWvH7RtG2e09Y8nCFe3PP98eDz44HjjcLH55JOy92OUlIQOCiXYY4/NCWPgQOjSJcz3AamqxxOEq1v8PghXztq18PrrmxPGq6/CmjVh2Y47hrG+33wTNmzYvI33YJuduO6DcM65GtGyJRx0UJgglCbmzdtcLfXPf4Z5ydatgzFjoLQUunXbPLVr5w3h2fIShMs/XoJwVdSoUfb9RbVsGQZRSk4aialrV2jdOpeR5h8vQTjn6rV0Pdh26QJvvRUavT/6qOL0n//A11+X3WbbbVMnjsRjQ+qDyhOEc67OGzcORo0K1UoJLVqE+a1bw157hak8szDYUvnEsXhxSCxPPAHffVd2mx/9qGLiSEydOkHjevSrWo/eiqs3br897ghcHZNoiK7qVUxS6GSwfXsoLq64fNMm+PTTsokj8fyVV8I44Mkj9BUUhCSRrgprhx1CdVhd4W0QzjlXTRs2wNKl6auwPvus7PpNm4Zqr3RVWHE0oHsbhKtbnnwyPB55ZLxxOJdBkyahe5Cdd069fP360DaSqgpr1qwwDGyyli3TJ49u3aBVq7Lr5/reD08QLv/ccEN49ATh6rjmzaFXrzCl8tVXFRPHRx/Bhx/CCy9UbEBv125zsli3Dp57bnMbyZIloR0Gai5JeIJwzrmYtG4Ne+8dpvLMwoh+5ds+Pvoo3Fn+/vsVt1m3LpQoPEE451w9JoUxNTp0CN2kl5fu3o+PP665GOpQe7pzzrmEdANo1uTAmp4gnHOuDho3LtzrkSxx70dN8Soml3/uvTfuCJzLe9W996MqPEG4/NOpU+Z1nHOMGJHb3mq9isnln4ceCpNzLlZegnD5529/C4/HHx9vHM41cF6CcM45l5InCOeccyl5gnDOOZdSThOEpMMlLZC0SNJFKZZL0l+i5W9J6pO0bLGkeZJKJHkXrc45V8ty1kgtqQC4BTgEKAVmSXrCzN5JWm0I0D2a9gX+Fj0mHGhmK3MVo8tTDz8cdwTOOXJbgugLLDKzD83sO+BBYFi5dYYB/7DgNaCtpB/lMCZXFyRGcHHOxSqXCWInYGnS69JoXrbrGDBF0hxJo9IdRNIoSbMlzV6xYkUNhO1iN2FCmJxzscplgkg1LlL5vgcrW2eAmfUhVEOdJWlQqoOY2XgzKzKzog4dOlQ/Wpc/PEE4lxdymSBKgeQ+EzoCy7Jdx8wSj8uBRwlVVs4552pJLu+kngV0l9QN+AQ4AfhFuXWeAM6W9CChcfpLM/tU0tZAIzNbEz0/FPhjDmNl8ODBFeYdd9xxnHnmmaxbt46hQ4dWWD5y5EhGjhzJypUrGT58eIXlv/71rzn++ONZunQpJ598coXl559/PkceeSQLFixg9OjRFZZfdtllHHzwwZSUlDB27NgKy6+66ir69+/PjBkzuOSSSyosv+mmmygsLOT555/nyiuvrLD89ttvp2fPnjz55JPckBjFLcm9995Lp06deOihh/hb4u7mJA8//DDt27dnwoQJTEhxxj958mRatGjBrbfeyqRJkyosnzp1KgDXX389Tz311Oa4S0po1KgRe0Wvr7jiCl544YUy27Zr145HHnkEgIsvvphXX321zPKOHTty3333ATB27FhKSkrKLO/Rowfjx48HYNSoUSxcuLDM8sLCQm666SYATjrpJEpLS8ss79evH1dffTUAxx57LKtWrSqz/KCDDuK3v/0tAEOGDGH9+vVllh9xxBFccMEFgH/38um7B9C8eXOeeeYZoO589xLvp6blLEGY2UZJZwPPAgXA3WY2X9KYaPltwGRgKLAIWAecGm2+PfCowujdjYH7zezfuYrVOedcRbJUQxLVUUVFRTZ7tt8yUeclzqhzdFbknNtM0hwzK0q1zDvrc/ln8uS4I3DO4QnC5aPyw2Q552LhfTG5/HPrrWFyzsXKE4TLP5Mmhck5FytPEM4551LyBOGccy4lTxDOOedS8gThnHMupXp1o5ykFcCSuOPIkTbAl3EHEamNWGr6GFu6v+puX5Xtanrd9kBDG0+lof2f1MRxuphZ6p5OzcynOjAB4+OOoTZjqeljbOn+qrt9Vbar6XWB2XF8P+KcGtr/Sa6P41VMdceTcQeQpDZiqeljbOn+qrt9VbbL1boNST59LrUVS86OU6+qmJxzm0mabWn62HEuG16CcK7+Gh93AK5u8xKEc865lLwE4ZxzLiVPEM4551LyBOGccy4lTxDONRCSBkt6WdJtkgbHHY/Lf54gnKvDJN0tabmkt8vNP1zSAkmLJF0UzTZgLdAMKK3tWF3d41cxOVeHSRpE+NH/h5ntEc0rABYChxASwSzgROA9M9skaXvgRjMbEVPYro7wEoRzdZiZTQM+Lze7L7DIzD40s++AB4FhZrYpWv4F0LQWw3R1lI9J7Vz9sxOwNOl1KbCvpJ8BhwFtgb/GEJerYzxBOFf/KMU8M7N/Af+q7WBc3eVVTM7VP6VAp6TXHYFlMcXi6jBPEM7VP7OA7pK6SdoKOAF4IuaYXB3kCcK5OkzSA8CrQE9JpZJON7ONwNnAs8C7wCQzmx9nnK5u8stcnXPOpeQlCOeccyl5gnDOOZeSJwjnnHMpeYJwzjmXkicI55xzKXmCcM45l5InCFejJJmkG5JeXyDp8hra9wRJw2tiXxmO83NJ70p6sdz8rpLWSyqR9E40rkLs/0OSjpbUO0f7bi7pJUkF0ft/O/NWGfd5hKQ/1ER8Lrdi/3K7eudb4GeS2scdSLKoC+xsnQ6caWYHplj2gZkVAnsBvYGjszx+Lvs9OzqKJWtViOc04F9m9n1Vg6rE08BRklrU4D5dDniCcDVtIzAe+O/yC8qXACStjR4HR2epkyQtlHSNpBGSZkqaJ2mXpN0cHI2KtlDSEdH2BZKukzRL0luSRift90VJ9wPzUsRzYrT/tyVdG837HTAQuE3SdeneZHS38gxgV0m/io49V9IjiR++6P3eGJVErpXUV9IMSW9Gjz2j9UZKekzSk5I+knS2pPOi9V6TtG203i6S/i1pTvQZ9JLUHzgKuC4q2eySar008RwQbVMSHatVirc6Ang83ecQ7fegaPt50QBGTaP5QyW9J+kVSX+R9FT02RkwFTiisv26PGBmPvlUYxNh8JrWwGKgDXABcHm0bAIwPHnd6HEwsBr4EWGcgk+AP0TL/gu4KWn7fxNObLoTOqVrBowCLovWaQrMBrpF+/0a6JYizh2Bj4EOhF6N/wMcHS2bChSl2KYr8Hb0vAWhz6MhQLukda4EzkmK9ymgIHrdGmgcPT8YeCR6PhJYBLSK4vkSGBMt+zMwNnr+AtA9er4v8J80n2tl6yXH8yQwIHreMhFb0n62Aj5L9f6T5jUjdC3eI3r9D2Bs0vxu0fwHgKeSthsB3Bz399Wnyifv7tvVODP7StI/gHOB9VluNsvMPgWQ9AEwJZo/D0iu6plkYeCb9yV9CPQCDgX2SiqdtCEkkO+AmWb2UYrjFQNTzWxFdMyJwCDgsQxx7iKphDB85+Nm9kx0Jn4lYZyFloQ+kBL+aZurZ9oA90jqHm3fJGm9F81sDbBG0peEH+/E+99LUkugP/BP6YfevCsM+pPFesnxTAdujN77v8ys/DCk7QmJuzI9gY/MbGH0+h7gLEKS/TDps3+AkMgTlhOStMtjniBcrtwEvAH8PWneRqJqTYVfr62Sln2b9HxT0utNlP2elu88zAjjH5xjZsk/zEgaTChBpJJqzIRsJNogkk0glD7mShpJKLkkJB//CkIiOEZSV8KPaEKm998IWJ3i2OVlWu+HeMzsGklPA0OB1yQdbGbvJa27nlASqEy6zzHT59uM7E8eXEy8DcLlhJl9DkwiNPgmLAZ+HD0fRtkz6Gz9XFKjqF1iZ2AB4Yz915KaAEjqIWnrDPt5HThAUvuoAftE4KVqxAOhaujT6PiVjfPchlB9BqFaKWtm9hXwkaSfQ0iwkvaOFq+JYsi0XhmSdjGzeWZ2LaFarle5Y34BFEiqLEm8B3SVtGv0+mTC5/gesHOUCAGOL7ddD2CLr4hyueUJwuXSDYRqioQ7CD/KMwl14+nO7iuzgPAD9Ayhnv4b4E7gHeANhcswbydD6TiqzroYeBGYC7xhZpU2xlbit4SE8xzhhzGdPwFXS5oOVOWqqoQRwOmS5gLzCUkWwpjTF0YNxbtUsl55Y6MG+rmEs/lnUqwzhdBon5DoVrxUUilwJHAqoUprHqHEc5uZrQfOBP4t6RXg/whtKwkHEq5mcnnMu/t2zqUlaR/gPDM7uRrbtjSztVF14i3A+2b2Z0nbA/eb2UE1Ha+rWV6CcM6lZWZvAi+qaveRJPwqatCfT6heuz2a3xk4v2YidLnkJQjnnHMpeQnCOedcSp4gnHPOpeQJwjnnXEqeIJxzzqXkCcI551xKniCcc86l9P+miH0H/EMqkwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(num_params, [1 - acc for acc in test_accs], 'bo-')\n",
    "plt.plot(num_params, [1-inf_test_acc]*len(widths), 'k--', label='Infinite Width')\n",
    "plt.axvline(x=40000, color='r', linestyle='--')\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Number of Parameters (Log)\")\n",
    "plt.ylabel(\"Test Error (1 - Acc.)\")\n",
    "plt.title(\"Double Descent with ReLU Network on MNIST\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_tutorial",
   "language": "python",
   "name": "dl_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
